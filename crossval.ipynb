{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import time"
   ],
   "id": "b3f4cdcbc82bffe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ],
   "id": "ce76dec8f7a164dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "drive_path = \"/content/drive/MyDrive/Espectrogramas5a20/\"\n",
    "spectrogram_path = os.path.join(drive_path, \"Ramphastidae\")\n",
    "path_to_save_model = os.path.join(drive_path, \"ModelosCNN\")\n",
    "os.makedirs(path_to_save_model, exist_ok=True)\n",
    "number_epochs = 80\n",
    "filtered_files_path = \"/content/drive/MyDrive/filtered_files_google_driveR.txt\"\n",
    "start_time = time.time()"
   ],
   "id": "3639f976dc6ac201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BirdCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 5, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 5, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(8),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, 256, 512)\n",
    "            features = self.features(dummy_input)\n",
    "            feature_size = features.view(1, -1).size(1)\n",
    "            print(feature_size)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "transformar = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Grayscale(),\n",
    "    v2.Resize((256, 512)),\n",
    "    v2.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "class FilteredSpectrogramDataset(Dataset):\n",
    "    def __init__(self, filelist_path=\"filtered_files.txt\", transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        with open(filelist_path) as f:\n",
    "            for line in f:\n",
    "                img_path, class_dir = line.strip().split(',')\n",
    "                self.samples.append((img_path, class_dir))\n",
    "\n",
    "        self.classes = sorted(set([s[1] for s in self.samples]))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        self.preloaded_images = [None] * len(self.samples)\n",
    "\n",
    "        def load_image(i, img_path):\n",
    "            img = Image.open(img_path).convert('L').crop((316, 60, 2181, 986))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            self.preloaded_images[i] = img\n",
    "\n",
    "        print(\"Preloading images with threading...\")\n",
    "        with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "            list(tqdm(\n",
    "                executor.map(lambda x: load_image(x[0], x[1][0]),\n",
    "                           enumerate(self.samples)),\n",
    "                total=len(self.samples),\n",
    "                desc=\"Preloading images\"\n",
    "            ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.preloaded_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.preloaded_images[idx], self.class_to_idx[self.samples[idx][1]]"
   ],
   "id": "8f396a02a4bc7eac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = FilteredSpectrogramDataset(\n",
    "    filelist_path=filtered_files_path,\n",
    "    transform=transformar\n",
    ")\n",
    "\n",
    "print(f\"Total classes detected: {len(dataset.classes)}\")\n",
    "print(f\"Class names: {dataset.classes}\")\n",
    "print(f\"Original samples: {sum([len(files) for _, _, files in os.walk(spectrogram_path)])}\")\n",
    "print(f\"Filtered samples: {len(dataset)}\")"
   ],
   "id": "a9067f8d97eb3313"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_class_weights(dataset_obj):\n",
    "    \"\"\"Calculate class weights inversely proportional to class frequencies\"\"\"\n",
    "    # Cálculo de amostras por classe\n",
    "    class_counts = np.zeros(len(dataset_obj.classes))\n",
    "    for _, label in dataset_obj.samples:\n",
    "        class_counts[dataset_obj.class_to_idx[label]] += 1\n",
    "\n",
    "    # Computa pesos inversos e os normaliza\n",
    "    inverse_weights = 1. / class_counts\n",
    "    normalized_weights = inverse_weights / inverse_weights.sum() * len(class_counts)  # Normalize\n",
    "\n",
    "    print(f\"Class distribution: {dict(zip(dataset_obj.classes, class_counts))}\")\n",
    "    print(f\"Computed weights: {normalized_weights}\")\n",
    "\n",
    "    return torch.tensor(normalized_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Calcula pesos iniciais de classe\n",
    "class_weights = calculate_class_weights(dataset)\n",
    "smoothed_weights = torch.log1p(class_weights)\n",
    "print(f\"Smoothed weights: {smoothed_weights}\")\n",
    "\n",
    "sample, _ = next(iter(DataLoader(dataset, batch_size=1)))\n",
    "print(f\"Spectrogram shape: {sample.shape}\")\n",
    "\n",
    "def calculate_class_weights_for_split(train_indices, dataset_obj):\n",
    "    \"\"\"Calculate class weights for a specific training split\"\"\"\n",
    "    class_counts = np.zeros(len(dataset_obj.classes))\n",
    "\n",
    "    for idx in train_indices:\n",
    "        _, label_str = dataset_obj.samples[idx]\n",
    "        class_idx = dataset_obj.class_to_idx[label_str]\n",
    "        class_counts[class_idx] += 1\n",
    "\n",
    "    class_counts = np.maximum(class_counts, 1e-8)\n",
    "\n",
    "    inverse_weights = 1. / class_counts\n",
    "    normalized_weights = inverse_weights / inverse_weights.sum() * len(class_counts)\n",
    "\n",
    "    print(f\"Split class distribution: {dict(zip(dataset_obj.classes, class_counts.astype(int)))}\")\n",
    "    print(f\"Split computed weights: {normalized_weights}\")\n",
    "\n",
    "    return torch.tensor(normalized_weights, dtype=torch.float32).to(device)"
   ],
   "id": "faa3f1e6aaed0381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def print_fold_stats(train_indices, val_indices, dataset_obj):\n",
    "    \"\"\"Print class distribution for current fold\"\"\"\n",
    "    print(\"\\nFold Distribution:\")\n",
    "\n",
    "    for class_name in dataset_obj.classes:\n",
    "        train_count = sum(1 for idx in train_indices\n",
    "                          if dataset_obj.samples[idx][1] == class_name)\n",
    "        val_count = sum(1 for idx in val_indices\n",
    "                        if dataset_obj.samples[idx][1] == class_name)\n",
    "\n",
    "        total_count = train_count + val_count\n",
    "        val_percentage = (val_count / total_count * 100) if total_count > 0 else 0\n",
    "\n",
    "        print(f\"  {class_name}: {train_count} train, {val_count} val \"\n",
    "              f\"({val_percentage:.1f}% in validation)\")\n",
    "\n",
    "\n",
    "def train_single_fold(model, train_loader, val_loader, criterion, optimizer,\n",
    "                      scheduler, epochs, fold_num, patience=10):\n",
    "    \"\"\"Train and validate for a single fold\"\"\"\n",
    "    model.to(device)\n",
    "    best_acc = 0\n",
    "    best_model_state = None\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    fold_train_accs = []\n",
    "    fold_val_accs = []\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Fase de treino\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Fase de validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # Cálculo de métricas\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Salvar métricas para este epoch\n",
    "        fold_train_losses.append(avg_train_loss)\n",
    "        fold_val_losses.append(avg_val_loss)\n",
    "        fold_train_accs.append(train_acc)\n",
    "        fold_val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Fold {fold_num + 1} Epoch {epoch + 1}: \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.2f}%, \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "        # Salvar melhor modelo do fold\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            best_model_state = {\n",
    "                'fold': fold_num,\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict().copy(),\n",
    "                'optimizer_state_dict': optimizer.state_dict().copy(),\n",
    "                'scheduler_state_dict': scheduler.state_dict().copy(),\n",
    "                'val_acc': best_acc,\n",
    "                'class_names': dataset.classes,\n",
    "                'train_losses': fold_train_losses.copy(),\n",
    "                'val_losses': fold_val_losses.copy(),\n",
    "                'train_accs': fold_train_accs.copy(),\n",
    "                'val_accs': fold_val_accs.copy()\n",
    "            }\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                print(f\"Best validation accuracy: {best_acc:.2f}% at epoch {best_epoch + 1}\")\n",
    "                break\n",
    "    print(f\"Fold {fold_num + 1} completed: Best Val Acc {best_acc:.2f}% at epoch {best_epoch + 1}\")\n",
    "\n",
    "    if best_model_state is None:\n",
    "            best_model_state = {\n",
    "                'fold': fold_num,\n",
    "                'epoch': best_epoch,\n",
    "                'model_state_dict': model.state_dict().copy(),\n",
    "                'optimizer_state_dict': optimizer.state_dict().copy(),\n",
    "                'scheduler_state_dict': scheduler.state_dict().copy(),\n",
    "                'val_acc': best_acc,\n",
    "                'class_names': dataset.classes,\n",
    "                'train_losses': fold_train_losses.copy(),\n",
    "                'val_losses': fold_val_losses.copy(),\n",
    "                'train_accs': fold_train_accs.copy(),\n",
    "                'val_accs': fold_val_accs.copy()\n",
    "            }\n",
    "\n",
    "    return best_acc, best_model_state\n",
    "\n",
    "\n",
    "def cross_validate_model(dataset, num_epochs=number_epochs, n_splits=5, patience=10):\n",
    "    \"\"\"Perform k-fold cross-validation\"\"\"\n",
    "    labels = [dataset.class_to_idx[dataset.samples[i][1]] for i in range(len(dataset))]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Inicializa k-fold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    best_models = []\n",
    "    epochs_used = []\n",
    "    print(f\"\\nStarting {n_splits}-fold cross-validation with {len(dataset)} samples\")\n",
    "    print(f\"Number of classes: {len(dataset.classes)}\")\n",
    "    print(f\"Early stopping patience: {patience} epochs\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(range(len(dataset)), labels)):\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "\n",
    "        print_fold_stats(train_idx, val_idx, dataset)\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "        # Calcula pesos de classe for para esta parte específica do treino\n",
    "        fold_class_weights = calculate_class_weights_for_split(train_idx, dataset)\n",
    "        smoothed_weights = torch.log1p(fold_class_weights)\n",
    "\n",
    "        # Cria carregadores de dados\n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=32,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True)\n",
    "        val_loader = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True)\n",
    "\n",
    "        # Inicializa novo modelo e otimizador para cada fold\n",
    "        model = BirdCNN(num_classes=len(dataset.classes))\n",
    "        criterion = nn.CrossEntropyLoss(weight=smoothed_weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=0.01,\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=num_epochs,\n",
    "            pct_start=0.25\n",
    "        )\n",
    "\n",
    "        # Treina e valida este fold\n",
    "        fold_best_acc, fold_best_model_state = train_single_fold(\n",
    "            model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "            num_epochs, fold\n",
    "        )\n",
    "\n",
    "        fold_results.append(fold_best_acc)\n",
    "        best_models.append(fold_best_model_state)\n",
    "        epochs_used.append(fold_best_model_state['epoch'] + 1)  # +1 because epochs are 0-indexed\n",
    "\n",
    "        print(f\"Fold {fold + 1} completed - Best Val Acc: {fold_best_acc:.2f}% \"\n",
    "              f\"(used {epochs_used[-1]}/{num_epochs} epochs)\")\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    fold_accuracies = np.array(fold_results)\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    std_accuracy = np.std(fold_accuracies)\n",
    "\n",
    "    total_epochs_planned = n_splits * num_epochs\n",
    "    total_epochs_used = sum(epochs_used)\n",
    "    epochs_saved = total_epochs_planned - total_epochs_used\n",
    "\n",
    "    print(f\"Individual fold accuracies:\")\n",
    "    for i, acc in enumerate(fold_results):\n",
    "        print(f\"  Fold {i + 1}: {acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\nMean CV Accuracy: {mean_accuracy:.2f}% ± {std_accuracy:.2f}%\")\n",
    "    print(f\"Best fold: {np.argmax(fold_results) + 1} with {np.max(fold_results):.2f}%\")\n",
    "    print(f\"Worst fold: {np.argmin(fold_results) + 1} with {np.min(fold_results):.2f}%\")\n",
    "\n",
    "    # Salva o melhor modelo de todos os folds\n",
    "    best_fold_idx = np.argmax(fold_results)\n",
    "    best_model_state = best_models[best_fold_idx]\n",
    "\n",
    "    save_path = os.path.join(path_to_save_model, f\"best_cv_model_fold{best_fold_idx + 1}.pth\")\n",
    "    torch.save(best_model_state, save_path)\n",
    "    print(f\"\\nBest model saved from fold {best_fold_idx + 1} to: {save_path}\")\n",
    "\n",
    "    return mean_accuracy, best_model_state, fold_results, epochs_used"
   ],
   "id": "bfda221bab1283ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    cv_mean_accuracy, best_model, fold_accuracies, epochs_used = cross_validate_model(\n",
    "        dataset,\n",
    "        num_epochs=number_epochs,\n",
    "        n_splits=5,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal training time: {total_time:.2f} seconds ({total_time / 60:.2f} minutes)\")\n",
    "    print(f\"Final Mean Cross-Validation Accuracy: {cv_mean_accuracy:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during cross-validation: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ],
   "id": "fc4daec58bc09ba7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
